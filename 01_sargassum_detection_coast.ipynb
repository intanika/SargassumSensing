{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Detection of Sargassum on the coast and coastal waters</b>  \n",
    "Notebook for classifying and analyzing Sargassum in Bonaire with Sentinel-2 images\n",
    "\n",
    "* Decision Tree Classifier (DTC) and Maximum Likelihood Classifier (MLC) are employed\n",
    "* Training sites covering 8 different classes are used to extract pixel values (training samples) over all Sentinel-2 bands\n",
    "* 12 Sentinel bands and 8 spectral indices evaluated using Jeffries-Matusita distance (selected: NDVI, REP, B05 and B11) \n",
    "* 80:20 train-test ratio for splitting the training samples\n",
    "* K-Fold cross-validation performed for tuning the DTC model\n",
    "* MLC model developed with 4 different chi-square thresholds: 0% (base), 10%,20%,50%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rasterio as rio\n",
    "from rasterio import Affine\n",
    "from rasterio.mask import mask\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "import geopandas as gpd\n",
    "from joblib import dump,load\n",
    "from rasterstats import zonal_stats\n",
    "from tqdm import tqdm,tqdm_notebook\n",
    "\n",
    "#custom functions\n",
    "from Python.prep_raster import stack_bands,clip_raster,pixel_sample,computeIndexStack\n",
    "from Python.data_treat import balance_sample,down_sample\n",
    "from Python.spec_analysis import transpose_df,jmd2df\n",
    "from Python.data_viz import specsign_plot,jmd_heatmap,ridgePlot,validation_curve_plot\n",
    "from Python.mlc import mlClassifier\n",
    "from Python.calc_acc import calc_acc\n",
    "from Python.pred_raster import stack2pred, dtc_pred_stack\n",
    "from Python.misc import get_feat_layer_order\n",
    "\n",
    "#sklearn functions\n",
    "from sklearn.model_selection import train_test_split,validation_curve\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#setup IO directories\n",
    "parent_dir = os.path.join(os.path.abspath('..'),'objective1')                  #change according to preference\n",
    "sub_dirs = ['fullstack','clippedstack','indexstack','predicted','stack2pred']\n",
    "make_dirs = [os.makedirs(os.path.join(parent_dir,name),exist_ok=True) for name in sub_dirs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Sentinel-2 data preparation</b>\n",
    "* Resample coarse bands to 10m resolution\n",
    "* Stack multiband images \n",
    "* Calculate spectral indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dates considered for classification and analysis \n",
    "dates = [20180304,20180309,20180314,20180319,20190108,20190128,20190212,20190304,20190309, \n",
    "         20190314,20190319,20190508,20190513,20190518,20190523,20190821,20191129]\n",
    "\n",
    "#band names\n",
    "bands = ['B01_60m','B02_10m','B03_10m','B04_10m','B05_20m','B06_20m',\n",
    "         'B07_20m','B08_10m','B8A_20m','B09_60m','B11_20m','B12_20m']\n",
    "\n",
    "#get product file paths according to dates and tile ID T19PEP (covers Bonaire)\n",
    "level2_dir = '...' #change according to preference\n",
    "level2_files = glob(level2_dir+\"/*.SAFE\")\n",
    "scene_paths=[file for date in dates for file in level2_files if str(date) in file and 'T19PEP' in file]\n",
    "\n",
    "#sort multiband image paths according to date\n",
    "image_collection ={}\n",
    "\n",
    "for scene in scene_paths:\n",
    "    date = re.findall(r\"(\\d{8})T\", scene)[0]\n",
    "    \n",
    "    #collect all .jp2 band images in SAFE directory\n",
    "    all_images = [f for f in glob(scene + \"*/**/*.jp2\", recursive=True)]\n",
    "    img_paths = [img_path for band in bands for img_path in all_images if band in img_path]\n",
    "    image_collection[date] = img_paths\n",
    "\n",
    "#check nr. of images per date\n",
    "for key in image_collection.keys():print(f'Date: {key} Images: {len(image_collection[key])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stack multiband images to a geotiff (!computationaly intensive)\n",
    "for date in tqdm(image_collection.keys(),position=0, leave=True):\n",
    "    ref10m= image_collection[date][1]                      #use band B02 (10m) as reference metadata\n",
    "    outfile = os.path.join(parent_dir,'fullstack',f'stack_{date}.tif')\n",
    "    stack_bands(image_collection[date],ref10m,outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crop multiband image stack and compute spectral indices\n",
    "roi_file = './data/boundaries/coastline_lacbay.geojson'                   #polygon for cropping image\n",
    "indices = ['NDVI','REP','FAI','GNDVI','NDVI_B8A','VB_FAH','SEI','SABI']   #list of indices used in the study\n",
    "\n",
    "stack_files = glob(parent_dir + \"/fullstack/*.tif\")\n",
    "for stack_file in tqdm(stack_files,position=0, leave=True):\n",
    "    filename = os.path.basename(stack_file).split('.')[0]\n",
    "    \n",
    "    #cropping\n",
    "    clip_outfile = os.path.join(parent_dir,'clippedstack',filename+\"_clipped.tif\")\n",
    "    clip_raster(stack_file,roi_file,clip_outfile,fill=True,nodat=0)\n",
    "    \n",
    "    #compute spectral indices\n",
    "    index_outfile = os.path.join(index_dir,filename+\"_index.tif\")\n",
    "    computeIndexStack(clip_outfile,indices,index_outfile)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Sample pixel values from multiband images based on training sites</b>  \n",
    "* Training scenes from 4,9,14 and 19 March 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get training sites and corresponding images\n",
    "train_sites = [f for f in glob(r\".\\data\\training_input\\objective1\\*_coast.geojson\")]     \n",
    "dates = [20190304,20190309,20190314,20190319]                                 \n",
    "stack_bands = [f for date in dates for f in glob(parent_dir+'/clipped*/*_clipped.tif') if str(date) in f]   \n",
    "index_bands = [f for date in dates for f in glob(parent_dir+'/index*/*_index.tif') if str(date) in f] \n",
    "\n",
    "#bands and indices to be sampled\n",
    "band_names = ['B01','B02','B03','B04','B05','B06','B07','B08','B8A','B09','B11','B12']\n",
    "indices = ['NDVI','REP','FAI','GNDVI','NDVI-B8A','VB-FAH','SEI','SABI']\n",
    "\n",
    "dataset = []\n",
    "for i in range(len(train_sites)):\n",
    "    \n",
    "    #sample multibands and spectral indices\n",
    "    df_bands= pixel_sample(stack_bands[i],train_sites[i],band_names)\n",
    "    df_indices= pixel_sample(index_bands[i],train_sites[i],indices)\n",
    "    df_sample = pd.concat([df_bands,df_indices],axis=1)\n",
    "    df_sample = df_sample.loc[:,~df_sample.columns.duplicated()]\n",
    "    \n",
    "    #downsample based on floating Sargassum (Sf)\n",
    "    df_downsampled = down_sample(df_sample,'C','Sf')\n",
    "    dataset.append(df_downsampled)\n",
    "    \n",
    "#final dataset\n",
    "dataset=pd.concat(dataset,sort=False).reset_index(drop=True) \n",
    "dataset.to_csv(r'./data/training_input/csv/training_samples_20190304_20190319_sargassum.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Expore spectral signature</b>  \n",
    "* Jeffries-Matusita distance (JMD) used for feature selection ([reference](https://books.google.nl/books?id=RxHbb3enITYC&pg=PA52&lpg=PA52&dq=for+one+feature+and+two+classes+the+Bhattacharyya+distance+is+given+by&source=bl&ots=sTKLGl1POo&sig=ACfU3U2s7tv0LT9vfSUat98l4L9_dyUgeg&hl=nl&sa=X&ved=2ahUKEwiKgeHYwI7lAhWIIlAKHZfJAC0Q6AEwBnoECAkQAQ#v=onepage&q&f=false))\n",
    "* NDVI, REP, B05 and B11 are selected as input features for the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load training sample\n",
    "df = pd.read_csv('./data/training_input/csv/training_samples_20190304_20190319_sargassum.csv')\n",
    "\n",
    "#plot spectral signature focused on 4 subclasses\n",
    "specsign_plot(df,df.columns[4:16],classtype='C')\n",
    "\n",
    "#plot JMD heatmap for each band\n",
    "jmd_bands = [jmd2df(transpose_df(df,'C',band)) for band in df.columns[4:16]]\n",
    "jmd_heatmap(jmd_bands)\n",
    "\n",
    "#plot JMD heatmap for each spectral index\n",
    "jmd_indices = [jmd2df(transpose_df(df,'C',band)) for band in df.columns[16:]]\n",
    "jmd_heatmap(jmd_indices)\n",
    "\n",
    "#plot distribution of selected input features\n",
    "sns.set_style('white')\n",
    "ridgePlot(df[['C','NDVI','REP','B05','B11']],'C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Build classifiers</b>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load training sample\n",
    "df = pd.read_csv('./data/training_input/csv/training_samples_20190304_20190319_sargassum.csv')\n",
    "predictors = ['NDVI','REP','B05','B11']\n",
    "subset_df = df[['C']+predictors]\n",
    "\n",
    "#split into train and test datasets 80:20\n",
    "train,test = train_test_split(subset_df, train_size = 0.8,random_state=1,shuffle=True,stratify=np.array(subset_df['C']))\n",
    "train = train.sort_values(by='C',ascending=True) #sort labels\n",
    "\n",
    "#split pedictors from labels (for DTC)\n",
    "le = LabelEncoder()\n",
    "X_train,y_train = train[predictors],le.fit_transform(train['C'])\n",
    "X_test,y_test = test[predictors],le.fit_transform(test['C'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform k-fold (=10) cross-validation \n",
    "\n",
    "#parameters considered in this step\n",
    "max_depth = np.arange(1,40,2)                    \n",
    "min_samples_split = list(range(2, 100,10))         \n",
    "max_leaf_nodes= list(range(2, 50,5))              \n",
    "min_samples_leaf= list(range(1, 100,10))           \n",
    "min_impurity_decrease=[0,0.00005,0.0001,0.0002,0.0005,0.001,0.0015,0.002,0.005,0.01,0.02,0.05,0.08]   \n",
    "criterion = ['gini','entropy']\n",
    "\n",
    "#assign parameters to a dictionary\n",
    "params = {'max_depth':max_depth,'min_samples_split':min_samples_split,\n",
    "          'max_leaf_nodes':max_leaf_nodes,'min_samples_leaf':min_samples_leaf,\n",
    "          'min_impurity_decrease':min_impurity_decrease,'criterion':criterion}\n",
    "\n",
    "#plot validation curve\n",
    "fig,axs = plt.subplots(3,2,figsize=(10,8))\n",
    "axs = axs.ravel()\n",
    "dtc = DecisionTreeClassifier(random_state=1,criterion='entropy')                    #default model\n",
    "\n",
    "for (param_name,param_range),i in zip(params.items(),range(len(params.items()))):\n",
    "    train_scores,test_scores = validation_curve(dtc,X_train.values,y_train,cv=10,scoring='accuracy',\n",
    "                                                n_jobs=-1,param_range=param_range,param_name=param_name)\n",
    "    validation_curve_plot(train_scores,test_scores,param_range,param_name,axs[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train dtc model based on best parameters\n",
    "dtc = DecisionTreeClassifier(max_depth=5,random_state=2,criterion='entropy',min_samples_split=70,\n",
    "                             max_leaf_nodes=15,min_samples_leaf=40,min_impurity_decrease=0.01,max_features=4)\n",
    "dtc = dtc.fit(X_train,y_train)\n",
    "\n",
    "#export model as joblib file\n",
    "dump(dtc,r\".\\data\\models\\dtc_model_sargassum.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Maximum Likelihood Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train mlc model\n",
    "mlc = mlClassifier(train,'C')\n",
    "\n",
    "#export model as joblib file\n",
    "dump(mlc,r\".\\data\\models\\mlc_model_sargassum.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compute model accuracies (based on test split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load models\n",
    "dtc = load(r\".\\data\\models\\dtc_model_sargassum.joblib\")\n",
    "mlc = load(r\".\\data\\models\\mlc_model_sargassum.joblib\")\n",
    "\n",
    "#DTC model accuracy\n",
    "dtc_y_pred = dtc.predict(X_test)\n",
    "con_mat_dtc = calc_acc(le.inverse_transform(y_test),le.inverse_transform(dtc_y_pred))\n",
    "con_mat_dtc['classifier'] = 'DTC'\n",
    "\n",
    "#MLC model accuracies with chi-square threshold\n",
    "chi_table = {'MLC base':None,'MLC 10%':7.78,'MLC 20%':5.99,'MLC 50%':3.36}\n",
    "\n",
    "mlc_conmats = []\n",
    "for key,value in chi_table.items():\n",
    "    con_mat_mlc = mlc.classify_testdata(test,'C',threshold=value)\n",
    "    con_mat_mlc['classifier'] = key\n",
    "    mlc_conmats.append(con_mat_mlc)\n",
    "\n",
    "#export model accuracies\n",
    "mlc_conmats = pd.concat(mlc_conmats)\n",
    "model_acc = pd.concat([con_mat_dtc,mlc_conmats])\n",
    "model_acc.to_csv('./data/output/objective1/dtc_mlc_model_acc_obj1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Classification</b>  \n",
    "* create an image stack for prediction (stack2pred) for all scenes in objective1 folder\n",
    "* classify each stack2pred image with the DTC and MLC models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all multiband and spectral index images\n",
    "stack_bands =  glob(parent_dir+'/clipped*/*_clipped.tif')\n",
    "index_bands = glob(parent_dir+'/index*/*_index.tif')\n",
    "\n",
    "#get the order of the selected predictors in the multiband and spectral index images\n",
    "predictors = ['NDVI','REP','B05','B11']\n",
    "used_indices, used_bands = get_feat_layer_order(predictors)\n",
    "\n",
    "stack2pred_paths = []\n",
    "\n",
    "#create stack2pred rasters\n",
    "for band_image,index_image in zip(stack_bands,index_bands):\n",
    "    date = re.findall(r\"(\\d{8})\", band_image)[0]\n",
    "    outfile = os.path.join(f'{parent_dir}\\stack2pred',f'stack2pred_{date}.tif')\n",
    "    stack2pred_paths.append(outfile)\n",
    "\n",
    "    stack2pred(index_image,band_image,used_indices,used_bands,outfile)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load models\n",
    "dtc = load(r\".\\data\\models\\dtc_model_sargassum.joblib\")\n",
    "mlc = load(r\".\\data\\models\\mlc_model_sargassum.joblib\")\n",
    "\n",
    "#stack2pred image paths\n",
    "stack2pred_paths = glob(parent_dir+'*/stack2pred/stack2pred_*.tif')\n",
    "\n",
    "#classify all stack2pred images\n",
    "for path in stack2pred_paths:\n",
    "    \n",
    "    date = re.findall(r\"(\\d{8})\", path)[0]\n",
    "    \n",
    "    #predict multiple mlc with thresholds\n",
    "    mlc_out = f'{parent_dir}/predicted/mlc/mlc_{date}_multi.tif'\n",
    "    os.makedirs(os.path.dirname(mlc_out),exist_ok=True)\n",
    "    if not os.path.exists(mlc_out):\n",
    "        chi_probs = [None,7.78,5.99,3.36]\n",
    "        mlc_preds = np.array([mlc.classify_raster_gx(path,out_file=None,threshold=prob) for prob in chi_probs])\n",
    "\n",
    "    #export multilayer mlc image\n",
    "    with rio.open(path) as src:\n",
    "        profile = src.profile.copy()\n",
    "        profile.update({'dtype': rio.uint16})\n",
    "        with rio.open(mlc_out ,'w',**profile) as dst:\n",
    "            dst.write(mlc_preds.astype(rio.uint16))\n",
    "    \n",
    "    #predict and export DTC raster\n",
    "    dtc_out = f'{parent_dir}/predicted/dtc/dtc_{date}.tif'\n",
    "    os.makedirs(os.path.dirname(dtc_out),exist_ok=True)\n",
    "    if not os.path.exists(dtc_out):\n",
    "        dtc_pred_stack(dtc,path,dtc_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MLC class posterior probability raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stack2pred image paths\n",
    "stack2pred_paths = glob(parent_dir+'*/stack2pred/stack2pred_*.tif')\n",
    "\n",
    "#compute probabality raster\n",
    "for path in stack2pred_paths:\n",
    "    mlc_prob_out = f'{parent_dir}/predicted/mlc/mlc_{date}_prob.tif'\n",
    "    os.makedirs(os.path.dirname(mlc_out),exist_ok=True)\n",
    "    mlc.prob_rasters(path,mlc_prob_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>External validity</b>  \n",
    "* Classify DTC and MLC results for a scene taken on 2019-05-18\n",
    "* Validation samples only covers Non-Floating Sargassum (Non-Sf) and Floating Sargassum (Sf)\n",
    "* Floating Sargassum (Sf) pixel value = 3 in the DTC and MLC rasters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get file paths\n",
    "val_samples = gpd.read_file(r'./data/training_input/objective1/sf_validation_20190518.geojson')\n",
    "dtc_file = glob(parent_dir+'/predicted*/dtc/dtc*20190518*.tif')[0]\n",
    "mlc_file = glob(parent_dir+'/predicted*/mlc/mlc*20190518*.tif')[0]\n",
    "\n",
    "coords =  [(val_samples.geometry[i][0].x,val_samples.geometry[i][0].y) for i in range(len(val_samples))]\n",
    "with rio.open(dtc_file) as dtc_src, rio.open(mlc_file) as mlc_src:\n",
    "    \n",
    "    #sample from dtc raster\n",
    "    val_samples['DTC'] = [pt[0] for pt in dtc_src.sample(coords)]\n",
    "    \n",
    "    #sample from multilayer mlc raster\n",
    "    mlc_multi = pd.concat([pd.DataFrame(pt).T for pt in mlc_src.sample(coords)],ignore_index=True)\n",
    "    val_samples[['MLC base','MLC 10%','MLC 20%','MLC 50%']] = mlc_multi\n",
    "    \n",
    "#convert pixel values to 1 if Sf, else to 0 for others\n",
    "val_samples[val_samples.columns[-5:]] = (val_samples[val_samples.columns[-5:]]==3).astype(int)\n",
    "\n",
    "#compute classification (validation) accuracy \n",
    "df_val = pd.DataFrame(val_samples.drop(columns='geometry'))\n",
    "\n",
    "acc_val_dfs = []\n",
    "for pred in df_val.columns[df_val.columns!='label']:\n",
    "    acc = calc_acc(df_val['label'].values, df_val[pred].values)\n",
    "    acc['classifier'] = pred\n",
    "    acc_val_dfs.append(acc)\n",
    "acc_val_dfs = pd.concat(acc_val_dfs)\n",
    "acc_val_dfs.to_csv('./data/output/objective1/dtc_mlc_external_val_obj1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot model and validation accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = pd.read_csv('./data/output/objective1/dtc_mlc_model_acc_obj1.csv').set_index('Model')\n",
    "val_df  = pd.read_csv('./data/output/objective1/dtc_mlc_external_val_obj1.csv').set_index('Observed')\n",
    "\n",
    "acc2plot = {'Model accuracy (8 classes)':model_df.loc['PA','UA'].str[:4].astype(float),\n",
    "            'Model F1-score (Sf)':model_df.loc['Sf','F1-score'].astype(float),\n",
    "            'Validation accuracy (2 classes)':val_df.loc['PA','UA'].str[:4].astype(float),\n",
    "            'Validation F1-score (Sf)':val_df.loc['1','F1-score'].astype(float)}\n",
    "\n",
    "[plt.plot(val_df['classifier'].unique(),value,label=key) for key,value in acc2plot.items()]\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Comparative analysis</b>  \n",
    "* Compare Sargassum (Sf and Sl) classified area across different scenes for each model\n",
    "* Persisting missclassification occur between the two Sargassum classes and other coastal features, hence a mask was applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get classification result paths\n",
    "dtc_paths = glob(parent_dir+'/predicted*/dtc/dtc*.tif')\n",
    "mlc_paths = glob(parent_dir+'/predicted*/mlc/mlc*.tif')\n",
    "\n",
    "#load mask\n",
    "sl_mask = [gpd.read_file('./data/boundaries/sf_sl_mask.geojson').__geo_interface__['features'][0]['geometry']]\n",
    "sf_mask = [gpd.read_file('./data/boundaries/sf_sl_mask.geojson').__geo_interface__['features'][1]['geometry']]\n",
    "\n",
    "#collection of Sargassum classification results\n",
    "data = dict.fromkeys(['Date','Sl MLC Base','Sl MLC 10%','Sl MLC 20%','Sl MLC 50%','Sl DTC',\n",
    "                     'Sf MLC Base','Sf MLC 10%','Sf MLC 20%','Sf MLC 50%','Sf DTC'], [])\n",
    "\n",
    "for i in range(len(mlc_paths)):\n",
    "    date = re.findall(r\"(\\d{8})\", mlc_paths[i])\n",
    "    data['Date'] = data['Date']+ [str(pd.to_datetime(date)[0].date())]\n",
    "    \n",
    "    with rio.open(dtc_paths[i]) as dtc_src, rio.open(mlc_paths[i]) as mlc_src:\n",
    "        \n",
    "        #sf pixel count\n",
    "        dtc_img= mask(dataset=dtc_src,shapes=sf_mask,nodata=dtc_src.nodata,invert=True)[0]\n",
    "        data['Sf DTC'] = data['Sf DTC']+[np.unique(dtc_img, return_counts=True)[1][2]]\n",
    "        \n",
    "        mlc_imgs= mask(dataset=mlc_src,shapes=sf_mask,nodata=mlc_src.nodata,invert=True)[0]\n",
    "        for k,sf_mlc_key in enumerate(list(data.keys())[6:-1]): \n",
    "            data[sf_mlc_key] = data[sf_mlc_key]+ [[np.unique(mlc_img, return_counts=True)[1][2] for mlc_img in mlc_imgs][k]]\n",
    "        \n",
    "        #sl pixel count\n",
    "        dtc_img= mask(dataset=dtc_src,shapes=sl_mask,nodata=dtc_src.nodata,invert=False)[0]\n",
    "        data['Sl DTC'] = data['Sl DTC']+[np.unique(dtc_img, return_counts=True)[1][3]]\n",
    "        \n",
    "        mlc_imgs= mask(dataset=mlc_src,shapes=sl_mask,nodata=mlc_src.nodata,invert=False)[0]\n",
    "        for j,sl_mlc_key in enumerate(list(data.keys())[1:5]): \n",
    "            data[sl_mlc_key] = data[sl_mlc_key]+[[np.unique(mlc_img, return_counts=True)[1][3] for mlc_img in mlc_imgs][j]]\n",
    "\n",
    "#export data\n",
    "data = pd.DataFrame(data)\n",
    "data.to_csv('./data/output/objective1/classified_area_obj1.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot Sargassum classified area in 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data and subset only the 2019 results\n",
    "data = pd.read_csv('./data/output/objective1/classified_area_obj1.csv',index_col='Date')[4:]\n",
    "\n",
    "#plot Floating Sargassum (Sf) and Sargassum on land (Sl)\n",
    "fig,axs = plt.subplots(1,2,figsize=(20,8))\n",
    "axs[0].set_ylabel('Classified area (ha)')\n",
    "plt.tight_layout()\n",
    "fig.autofmt_xdate()\n",
    "\n",
    "plots = [axs[0].plot(data[col]/100) if 'Sf' in col else axs[1].plot(data[col]/100) for col in data.columns]\n",
    "legends = axs[0].legend(data.columns[:5],loc='upper right'),axs[1].legend(data.columns[5:],loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Sargassum coverage maps</b>  \n",
    "* Compute Sargassum coverage maps  for the invasions in March and May 2019 and March 2018\n",
    "* A 20mx20m grid was used to calculate the coverage for each scene\n",
    "* MLC 20% results were used for Floating Sargassum (Sf) coverage map\n",
    "* MLC 50% results were used for Sargassum on land (Sl) coverage map\n",
    "* Note that code below takes about 10 minutes to run (due to small grid tile size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get classification result paths\n",
    "mlc_paths = glob(parent_dir+'/predicted*/mlc/mlc*03*.tif')+glob(parent_dir+'/predicted*/mlc/mlc*05*.tif')\n",
    "\n",
    "#load mask and grid data\n",
    "mask_data = gpd.read_file('./data/boundaries/objective1/sf_sl_mask.geojson').__geo_interface__['features']\n",
    "grid_file = gpd.read_file(r'./data/boundaries/objective1/20mgrid.geojson')\n",
    "\n",
    "#collect geodataframes\n",
    "data = []\n",
    "\n",
    "for mlc_file in mlc_paths:\n",
    "    date = re.findall(r\"(\\d{8})\", mlc_file)[0]\n",
    "    with rio.open(mlc_file) as src:\n",
    "        \n",
    "        #iterate according to mask data (first item = sl, second item = sf)\n",
    "        #count number of pixel in each grid tile (computationaly intensive!)\n",
    "        for feat,label,val,inv,model in zip(mask_data,['sl','sf'],[4,3],[False,True],[3,2]):\n",
    "            img = mask(dataset=src,shapes=[feat['geometry']],nodata=src.nodata,invert=inv)[0][model]\n",
    "            zs = zonal_stats(grid_file,np.where(img==val,1,0),affine=src.transform,\n",
    "                            prefix=f'{label}_{date}_',stats='count',geojson_out=True,nodata=0)\n",
    "            zs_filter = list(filter(lambda x: x['properties'][f'{label}_{date}_count']!=0, zs))\n",
    "            data.append(gpd.GeoDataFrame.from_features(zs_filter,crs=grid_file.crs))\n",
    "\n",
    "#merge with grid file based on id\n",
    "grid_file_copy = grid_file.copy()\n",
    "for i in range(len(data)):\n",
    "    grid_file_copy = gpd.GeoDataFrame(grid_file_copy.merge(data[i][data[i].columns[1:]],on='id',how='outer'),\n",
    "                                  crs=grid_file.crs,geometry=grid_file.geometry).replace(np.nan,0)\n",
    "\n",
    "#calculate coverage for each grid tile \n",
    "sf_split = np.array_split(grid_file_copy[[i for i in grid_file_copy.columns if 'sf' in i ]],3,axis=1)\n",
    "sl_split = np.array_split(grid_file_copy[[i for i in grid_file_copy.columns if 'sl' in i ]],3,axis=1)\n",
    "scale_factor = (100/4/400) #(relative coverage of Sentinel-2 pixels in a 20x20m tile over 4 dates)\n",
    "sf_covr = [sf_split[i].sum(1)*scale_factor for i in range(len(sf_split))]\n",
    "sl_covr = [sl_split[i].sum(1)*scale_factor for i in range(len(sl_split))]\n",
    "\n",
    "#export coverage maps\n",
    "gdf_out = pd.concat([grid_file_copy[['geometry']]]+sf_covr+sl_covr,axis=1)\n",
    "gdf_out.columns = ['geometry','sf_mar2018','sf_mar2019','sf_may2019','sl_mar2018','sl_mar2019','sl_may2019']\n",
    "gdf_out = gdf_out[gdf_out[gdf_out.columns[1:]].sum(1)!=0]\n",
    "gdf_out.to_file(r'./data/output/objective1/sargassum_coverage_coast.geojson',driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
